---
title: "Multi-graph based hierarchical semantic fusion for cross-modal representation"
collection: publications
permalink: /publication/2010-10-01-paper-title-number-2
excerpt: 'Cross-Modal Retrieval; Multimodal Learning; Representation Learning'
date: 2010-10-01
venue: '2021 IEEE International Conference on Multimedia and Expo (ICME)'
paperurl: 'https://ieeexplore.ieee.org/abstract/document/9428194'
citation: 'Lei Zhu, Chengyuan Zhang, Jiayu Song, Liangchen Liu, Shichao Zhang, Yangding Li, Multi-graph based hierarchical semantic fusion for cross-modal representation, 2021 IEEE International Conference on Multimedia and Expo (ICME), 2021: 1-6 '
---

The main challenge of cross-modal retrieval is how to efficiently realize semantic alignment and reduce the heterogeneity gap. However, existing approaches ignore the multi-grained semantic knowledge learning from different modalities. To this end, this paper proposes a novel end-to-end cross-modal representation method, termed as Multi-Graph based Hierarchical Semantic Fusion (MG-HSF). This method is an integration of multi-graph hierarchical semantic fusion with cross-modal adversarial learning, which captures fine-grained and coarse-grained semantic knowledge from cross-modal samples, and generate modalities-invariant representations in a common subspace. To evaluate the performance, extensive experiments are conducted on three benchmarks. The experimental results show that our method is superior than the state-of-the-arts.
