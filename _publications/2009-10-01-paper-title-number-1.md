---
title: "HCMSL: Hybrid cross-modal similarity learning for cross-modal retrieval"
collection: publications
permalink: /publication/2009-10-01-paper-title-number-1
excerpt: 'Cross-Modal Retrieval; Multimodal Learning'
date: 2021-April-06
venue: 'ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)'
paperurl: 'https://dl.acm.org/doi/abs/10.1145/3412847'
citation: 'Chengyuan Zhang, Jiayu Song, Xiaofeng Zhu, Lei Zhu, Shichao Zhang, HCMSL: Hybrid cross-modal similarity learning for cross-modal retrieval, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 2021, 17(1s): 1-22'
---

The purpose of cross-modal retrieval is to find the relationship between different modal samples and to retrieve other modal samples with similar semantics by using a certain modal sample. As the data of different modalities presents heterogeneous low-level feature and semantic-related high-level features, the main problem of cross-modal retrieval is how to measure the similarity between different modalities. In this article, we present a novel cross-modal retrieval method, named Hybrid Cross-Modal Similarity Learning model (HCMSL for short). It aims to capture sufficient semantic information from both labeled and unlabeled cross-modal pairs and intra-modal pairs with same classification label. Specifically, a coupled deep fully connected networks are used to map cross-modal feature representations into a common subspace. Weight-sharing strategy is utilized between two branches of networks to diminish cross-modal heterogeneity. Furthermore, two Siamese CNN models are employed to learn intra-modal similarity from samples of same modality. Comprehensive experiments on real datasets clearly demonstrate that our proposed technique achieves substantial improvements over the state-of-the-art cross-modal retrieval techniques.
