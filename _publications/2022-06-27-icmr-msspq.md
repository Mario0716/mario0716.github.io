---
title: "MSSPQ: Multiple Semantic Structure-Preserving Quantization for Cross-Modal Retrieval"
collection: publications
permalink: /publication/2022-06-27-icmr-msspq
excerpt: 'Cross-Modal Retrieval; Multimodal Learning; Representation Learning'
date: 2022-June-27
venue: 'Proceedings of the 2022 International Conference on Multimedia Retrieval (ICMR)'
paperurl: 'https://dl.acm.org/doi/abs/10.1145/3512527.3531417'
citation: 'Lei Zhu, Liewu Cai, Jiayu Song, Xinghui Zhu, Chengyuan Zhang, Shichao Zhang, MSSPQ: Multiple Semantic Structure-Preserving Quantization for Cross-Modal Retrieval, Proceedings of the 2022 International Conference on Multimedia Retrieval. 2022: 631-638'
---

Cross-modal hashing is a hot issue in the multimedia community, which is to generate compact hash code from multimedia content for efficient cross-modal search. Two challenges, i.e., (1) How to efficiently enhance cross-modal semantic mining is essential for cross-modal hash code learning, and (2) How to combine multiple semantic correlations learning to improve the semantic similarity preserving, cannot be ignored. To this end, this paper proposed a novel end-to-end cross-modal hashing approach, named Multiple Semantic Structure-Preserving Quantization (MSSPQ) that is to integrate deep hashing model with multiple semantic correlation learning to boost hash learning performance. The multiple semantic correlation learning consists of inter-modal and intra-modal pairwise correlation learning and Cosine correlation learning, which can comprehensively capture cross-modal consistent semantics and realize semantic similarity preserving. Extensive experiments are conducted on three multimedia datasets, which confirms that the proposed method outperforms the baselines.
